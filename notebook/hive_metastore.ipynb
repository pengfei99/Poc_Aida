{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143604b6-5931-40ab-8b4b-5c926260902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1240695d-53f7-4460-9571-5c99d6281351",
   "metadata": {},
   "outputs": [],
   "source": [
    "local=False\n",
    "if local:\n",
    "    spark=SparkSession.builder.master(\"local[4]\") \\\n",
    "                  .appName(\"aida_poc_etl\").getOrCreate()\n",
    "else:\n",
    "    spark=SparkSession.builder \\\n",
    "                      .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "                      .appName(\"aida_poc_etl\") \\\n",
    "                      .config(\"spark.kubernetes.container.image\",os.environ[\"IMAGE_NAME\"]) \\\n",
    "                      .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\",os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "                      .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "                      .config(\"spark.executor.instances\", \"10\") \\\n",
    "                      .config(\"spark.executor.memory\",\"16g\") \\\n",
    "                      .config(\"spark.driver.memory\",\"32g\") \\\n",
    "                      .config('spark.jars.packages','org.postgresql:postgresql:42.2.24') \\\n",
    "                      .enableHiveSupport() \\\n",
    "                      .getOrCreate()\n",
    "    \n",
    "def set_log_level(spark_session,log_level:str):\n",
    "    logger = spark_session.sparkContext._jvm.org.apache.log4j\n",
    "    if log_level==\"INFO\":\n",
    "        logger_level = logger.Level.INFO\n",
    "    elif log_level==\"WARN\":\n",
    "        logger_level = logger.Level.WARN\n",
    "    elif log_level==\"ERROR\":\n",
    "        logger_level = logger.Level.ERROR\n",
    "    else:\n",
    "        raise ValueError(\"The log_level must be INFO, WARN or ERROR\")\n",
    "    logger.LogManager.getLogger(\"org\").setLevel(logger_level)\n",
    "    logger.LogManager.getLogger(\"akka\").setLevel(logger_level)\n",
    "    \n",
    "set_log_level(spark,\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c419b-2e38-4ee7-8d6f-4a3e7faefcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir=\"s3a://projet-poc-aida/rp\"\n",
    "parquet_file_name=\"individus_snappy_parquet\"\n",
    "data_path=f\"{work_dir}/{parquet_file_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab0756-bd7a-41f6-bf38-70289420610f",
   "metadata": {},
   "source": [
    "# Step 1: Prepare source dataframe\n",
    "\n",
    "Use spark context to read a parquet file and return a data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1731e55-b6f7-4b93-ae85-0695405013e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet=spark.read.parquet(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6b079-e3d1-4503-9c30-aa11d888d8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b3e5b-ecd3-4cd6-b674-c619c84fbc19",
   "metadata": {},
   "source": [
    "# Step2: Create a table in hive metastore\n",
    "\n",
    "Use the spark dataframe to create a hive table in the hive metastore. So we can reuse it for later. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae3b20b-ffcb-4860-bd98-b1eb0684d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name=\"pengfei_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5df66c6-e677-4081-84ab-9c302030a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_str = ', '.join([' '.join(x) for x in df_parquet.dtypes])\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {table_name}\n",
    "({schema_str})\n",
    "STORED as parquet LOCATION '{data_path}'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe9ee03-c83a-43db-b8a3-1a65f3c2c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show tables;').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155fbab0-b7ab-425b-b187-bada38a8b5fd",
   "metadata": {},
   "source": [
    "Now your hive table has been created. In the backgroud, if you enabled the listener, the metadata of this hive table will be uploaded to our [data catalog](https://atlas.lab.sspcloud.fr/index.html#!/search). So you can find all your hive table easily even you don't have notebook anymore.\n",
    "\n",
    "You can try to use the search engine of our [data catalog](https://atlas.lab.sspcloud.fr/index.html#!/search) to find your table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4480a72-9c1c-4d91-bb30-2b7503d0b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"SELECT * FROM {table_name} limit 5\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef6130d-146e-47fc-b264-a08418a33f9c",
   "metadata": {},
   "source": [
    "# Step 3. Drop table \n",
    "\n",
    "You can delete your table if you don't need it anymore. You will notice the metadata of the deleted table are `removed` from the data catalog too.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4d2753-af8b-4203-b4c4-bd79b7c39954",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"drop table if exists {table_name}\"\"\").show()\n",
    "spark.sql('show tables;').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e1c0d2-4dbf-447a-b80b-34a52d2d07a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

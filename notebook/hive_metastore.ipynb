{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "143604b6-5931-40ab-8b4b-5c926260902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1240695d-53f7-4460-9571-5c99d6281351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e6808f69-1307-411c-858a-c8ca6c63b0d1;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.2.24 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      ":: resolution report :: resolve 154ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.2.24 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e6808f69-1307-411c-858a-c8ca6c63b0d1\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/6ms)\n",
      "2022-09-04 10:13:56,453 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2022-09-04 10:13:56,803 INFO spark.SparkContext: Running Spark version 3.3.0\n",
      "2022-09-04 10:13:56,827 INFO resource.ResourceUtils: ==============================================================\n",
      "2022-09-04 10:13:56,827 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2022-09-04 10:13:56,828 INFO resource.ResourceUtils: ==============================================================\n",
      "2022-09-04 10:13:56,828 INFO spark.SparkContext: Submitted application: aida_poc_etl\n",
      "2022-09-04 10:13:56,849 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 16384, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2022-09-04 10:13:56,865 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "2022-09-04 10:13:56,867 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2022-09-04 10:13:56,927 INFO spark.SecurityManager: Changing view acls to: jovyan\n",
      "2022-09-04 10:13:56,927 INFO spark.SecurityManager: Changing modify acls to: jovyan\n",
      "2022-09-04 10:13:56,927 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2022-09-04 10:13:56,928 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2022-09-04 10:13:56,928 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "2022-09-04 10:13:57,265 INFO util.Utils: Successfully started service 'sparkDriver' on port 36689.\n",
      "2022-09-04 10:13:57,293 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2022-09-04 10:13:57,328 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2022-09-04 10:13:57,347 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2022-09-04 10:13:57,347 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2022-09-04 10:13:57,351 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2022-09-04 10:13:57,372 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-4c6be2d1-002f-4527-beec-1b4b3c7ddec1\n",
      "2022-09-04 10:13:57,391 INFO memory.MemoryStore: MemoryStore started with capacity 19.0 GiB\n",
      "2022-09-04 10:13:57,406 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2022-09-04 10:13:57,461 INFO util.log: Logging initialized @3162ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2022-09-04 10:13:57,576 INFO server.Server: jetty-9.4.46.v20220331; built: 2022-03-31T16:38:08.030Z; git: bc17a0369a11ecf40bb92c839b9ef0a8ac50ea18; jvm 11.0.16+8-post-Ubuntu-0ubuntu120.04\n",
      "2022-09-04 10:13:57,594 INFO server.Server: Started @3298ms\n",
      "2022-09-04 10:13:57,629 INFO server.AbstractConnector: Started ServerConnector@424fbecd{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2022-09-04 10:13:57,629 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2022-09-04 10:13:57,649 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@688bb84b{/,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:57,671 INFO spark.SparkContext: Added JAR file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.24.jar at spark://jupyter-4061-0.jupyter-4061.user-pengfei.svc.cluster.local:36689/jars/org.postgresql_postgresql-42.2.24.jar with timestamp 1662286436795\n",
      "2022-09-04 10:13:57,671 INFO spark.SparkContext: Added JAR file:///home/jovyan/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar at spark://jupyter-4061-0.jupyter-4061.user-pengfei.svc.cluster.local:36689/jars/org.checkerframework_checker-qual-3.5.0.jar with timestamp 1662286436795\n",
      "2022-09-04 10:13:57,674 INFO spark.SparkContext: Added file file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.24.jar at spark://jupyter-4061-0.jupyter-4061.user-pengfei.svc.cluster.local:36689/files/org.postgresql_postgresql-42.2.24.jar with timestamp 1662286436795\n",
      "2022-09-04 10:13:57,675 INFO util.Utils: Copying /home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.24.jar to /tmp/spark-903f0e4c-347f-4016-91c7-09def1a96994/userFiles-a63d3323-c6f2-4cb9-bd02-f20b0a889ac2/org.postgresql_postgresql-42.2.24.jar\n",
      "2022-09-04 10:13:57,690 INFO spark.SparkContext: Added file file:///home/jovyan/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar at spark://jupyter-4061-0.jupyter-4061.user-pengfei.svc.cluster.local:36689/files/org.checkerframework_checker-qual-3.5.0.jar with timestamp 1662286436795\n",
      "2022-09-04 10:13:57,690 INFO util.Utils: Copying /home/jovyan/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar to /tmp/spark-903f0e4c-347f-4016-91c7-09def1a96994/userFiles-a63d3323-c6f2-4cb9-bd02-f20b0a889ac2/org.checkerframework_checker-qual-3.5.0.jar\n",
      "2022-09-04 10:13:57,759 INFO k8s.SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file\n",
      "2022-09-04 10:13:58,856 INFO util.Utils: Using initial executors = 10, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "2022-09-04 10:13:59,009 INFO k8s.ExecutorPodsAllocator: Going to request 5 executors from Kubernetes for ResourceProfile Id: 0, target: 10, known: 0, sharedSlotFromPendingPods: 2147483647.\n",
      "2022-09-04 10:13:59,078 INFO submit.KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : spark-env.sh,log4j2.properties\n",
      "2022-09-04 10:13:59,080 INFO submit.KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : spark-env.sh,log4j2.properties\n",
      "2022-09-04 10:13:59,095 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2022-09-04 10:13:59,136 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35893.\n",
      "2022-09-04 10:13:59,137 INFO netty.NettyBlockTransferService: Server created on jupyter-4061-0.jupyter-4061.user-pengfei.svc.cluster.local:35893\n",
      "2022-09-04 10:13:59,138 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2022-09-04 10:13:59,147 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, jupyter-4061-0.jupyter-4061.user-pengfei.svc.cluster.local, 35893, None)\n",
      "2022-09-04 10:13:59,153 INFO storage.BlockManagerMasterEndpoint: Registering block manager jupyter-4061-0.jupyter-4061.user-pengfei.svc.cluster.local:35893 with 19.0 GiB RAM, BlockManagerId(driver, jupyter-4061-0.jupyter-4061.user-pengfei.svc.cluster.local, 35893, None)\n",
      "2022-09-04 10:13:59,159 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, jupyter-4061-0.jupyter-4061.user-pengfei.svc.cluster.local, 35893, None)\n",
      "2022-09-04 10:13:59,160 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, jupyter-4061-0.jupyter-4061.user-pengfei.svc.cluster.local, 35893, None)\n",
      "2022-09-04 10:13:59,176 INFO util.Utils: Using initial executors = 10, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "2022-09-04 10:13:59,177 INFO spark.ExecutorAllocationManager: Dynamic allocation is enabled without a shuffle service.\n",
      "2022-09-04 10:13:59,216 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@688bb84b{/,null,STOPPED,@Spark}\n",
      "2022-09-04 10:13:59,216 INFO submit.KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : spark-env.sh,log4j2.properties\n",
      "2022-09-04 10:13:59,217 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21aeb91e{/jobs,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,217 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@57e1ca8e{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,218 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ab8a77f{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,219 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f82aeeb{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,219 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2022-09-04 10:13:59,220 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@66ed1384{/stages,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,221 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46f7a0fb{/stages/json,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,222 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@39bc1d87{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,223 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e62b403{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,223 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5670fa73{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,224 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7956df00{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,225 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6b6ff038{/storage,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,226 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b612fa9{/storage/json,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,226 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ce6a352{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,227 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@282921c6{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,228 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1949b6{/environment,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,229 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ba22fbf{/environment/json,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,229 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4c38a13d{/executors,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,230 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1cc26192{/executors/json,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,232 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19b502dd{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,233 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1bb7edb5{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,241 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5116db74{/static,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,241 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18a713d5{/,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,242 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@16a253eb{/api,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,243 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@590eb541{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,244 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@478f049d{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,249 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f0b56eb{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2022-09-04 10:13:59,273 INFO submit.KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : spark-env.sh,log4j2.properties\n",
      "2022-09-04 10:13:59,275 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2022-09-04 10:13:59,328 INFO submit.KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : spark-env.sh,log4j2.properties\n",
      "2022-09-04 10:13:59,330 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2022-09-04 10:13:59,383 INFO submit.KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : spark-env.sh,log4j2.properties\n",
      "2022-09-04 10:13:59,386 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2022-09-04 10:13:59,446 INFO k8s.ExecutorPodsAllocator: Going to request 5 executors from Kubernetes for ResourceProfile Id: 0, target: 10, known: 5, sharedSlotFromPendingPods: 2147483642.\n",
      "2022-09-04 10:13:59,448 INFO submit.KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : spark-env.sh,log4j2.properties\n",
      "2022-09-04 10:13:59,449 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2022-09-04 10:13:59,497 INFO submit.KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : spark-env.sh,log4j2.properties\n",
      "2022-09-04 10:13:59,498 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2022-09-04 10:13:59,542 INFO submit.KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : spark-env.sh,log4j2.properties\n",
      "2022-09-04 10:13:59,545 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2022-09-04 10:13:59,600 INFO submit.KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : spark-env.sh,log4j2.properties\n",
      "2022-09-04 10:13:59,602 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2022-09-04 10:13:59,649 INFO submit.KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : spark-env.sh,log4j2.properties\n",
      "2022-09-04 10:13:59,652 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2022-09-04 10:14:03,288 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.86.188:55046) with ID 2,  ResourceProfileId 0\n",
      "2022-09-04 10:14:03,295 INFO dynalloc.ExecutorMonitor: New executor 2 has registered (new total is 1)\n",
      "2022-09-04 10:14:03,352 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.126.37:35036) with ID 7,  ResourceProfileId 0\n",
      "2022-09-04 10:14:03,353 INFO dynalloc.ExecutorMonitor: New executor 7 has registered (new total is 2)\n",
      "2022-09-04 10:14:03,438 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.233.86.188:35601 with 9.4 GiB RAM, BlockManagerId(2, 10.233.86.188, 35601, None)\n",
      "2022-09-04 10:14:03,478 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.233.126.37:37807 with 9.1 GiB RAM, BlockManagerId(7, 10.233.126.37, 37807, None)\n",
      "2022-09-04 10:14:03,607 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.112.47:41430) with ID 10,  ResourceProfileId 0\n",
      "2022-09-04 10:14:03,608 INFO dynalloc.ExecutorMonitor: New executor 10 has registered (new total is 3)\n",
      "2022-09-04 10:14:03,690 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.118.44:50978) with ID 5,  ResourceProfileId 0\n",
      "2022-09-04 10:14:03,691 INFO dynalloc.ExecutorMonitor: New executor 5 has registered (new total is 4)\n",
      "2022-09-04 10:14:03,731 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.233.112.47:34941 with 9.4 GiB RAM, BlockManagerId(10, 10.233.112.47, 34941, None)\n",
      "2022-09-04 10:14:03,835 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.233.118.44:43659 with 9.4 GiB RAM, BlockManagerId(5, 10.233.118.44, 43659, None)\n",
      "2022-09-04 10:14:04,086 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.121.16:43342) with ID 6,  ResourceProfileId 0\n",
      "2022-09-04 10:14:04,088 INFO dynalloc.ExecutorMonitor: New executor 6 has registered (new total is 5)\n",
      "2022-09-04 10:14:04,239 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.233.121.16:35789 with 9.4 GiB RAM, BlockManagerId(6, 10.233.121.16, 35789, None)\n",
      "2022-09-04 10:14:04,482 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.127.48:45464) with ID 1,  ResourceProfileId 0\n",
      "2022-09-04 10:14:04,483 INFO dynalloc.ExecutorMonitor: New executor 1 has registered (new total is 6)\n",
      "2022-09-04 10:14:04,644 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.233.127.48:37393 with 9.1 GiB RAM, BlockManagerId(1, 10.233.127.48, 37393, None)\n",
      "2022-09-04 10:14:04,926 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.127.151:44130) with ID 8,  ResourceProfileId 0\n",
      "2022-09-04 10:14:04,926 INFO dynalloc.ExecutorMonitor: New executor 8 has registered (new total is 7)\n",
      "2022-09-04 10:14:04,948 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.127.101:40130) with ID 3,  ResourceProfileId 0\n",
      "2022-09-04 10:14:04,949 INFO dynalloc.ExecutorMonitor: New executor 3 has registered (new total is 8)\n",
      "2022-09-04 10:14:04,960 INFO k8s.KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "2022-09-04 10:14:05,101 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.233.127.151:41249 with 9.1 GiB RAM, BlockManagerId(8, 10.233.127.151, 41249, None)\n",
      "2022-09-04 10:14:05,105 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.233.127.101:41771 with 9.1 GiB RAM, BlockManagerId(3, 10.233.127.101, 41771, None)\n",
      "2022-09-04 10:14:05,977 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.113.227:50710) with ID 4,  ResourceProfileId 0\n",
      "2022-09-04 10:14:05,978 INFO dynalloc.ExecutorMonitor: New executor 4 has registered (new total is 9)\n",
      "2022-09-04 10:14:06,136 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.233.113.227:35407 with 9.1 GiB RAM, BlockManagerId(4, 10.233.113.227, 35407, None)\n",
      "2022-09-04 10:14:06,335 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.113.23:35376) with ID 9,  ResourceProfileId 0\n",
      "2022-09-04 10:14:06,336 INFO dynalloc.ExecutorMonitor: New executor 9 has registered (new total is 10)\n",
      "2022-09-04 10:14:06,483 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.233.113.23:42577 with 9.1 GiB RAM, BlockManagerId(9, 10.233.113.23, 42577, None)\n"
     ]
    }
   ],
   "source": [
    "local=False\n",
    "if local:\n",
    "    spark=SparkSession.builder.master(\"local[4]\") \\\n",
    "                  .appName(\"aida_poc_etl\").getOrCreate()\n",
    "else:\n",
    "    spark=SparkSession.builder \\\n",
    "                      .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "                      .appName(\"aida_poc_etl\") \\\n",
    "                      .config(\"spark.kubernetes.container.image\",os.environ[\"IMAGE_NAME\"]) \\\n",
    "                      .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\",os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "                      .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "                      .config(\"spark.executor.instances\", \"10\") \\\n",
    "                      .config(\"spark.executor.memory\",\"16g\") \\\n",
    "                      .config(\"spark.driver.memory\",\"32g\") \\\n",
    "                      .config('spark.jars.packages','org.postgresql:postgresql:42.2.24') \\\n",
    "                      .enableHiveSupport() \\\n",
    "                      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "883deec4-6157-43e3-9d88-ee0f1dd444e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_log_level(spark_session,log_level:str):\n",
    "    logger = spark_session.sparkContext._jvm.org.apache.log4j\n",
    "    if log_level==\"INFO\":\n",
    "        logger_level = logger.Level.INFO\n",
    "    elif log_level==\"WARN\":\n",
    "        logger_level = logger.Level.WARN\n",
    "    elif log_level==\"ERROR\":\n",
    "        logger_level = logger.Level.ERROR\n",
    "    else:\n",
    "        raise ValueError(\"The log_level must be INFO, WARN or ERROR\")\n",
    "    logger.LogManager.getLogger(\"org\").setLevel(logger_level)\n",
    "    logger.LogManager.getLogger(\"akka\").setLevel(logger_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12c0953a-1150-4126-b640-fcf496b89572",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_log_level(spark,\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae8c419b-2e38-4ee7-8d6f-4a3e7faefcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "work_dir=\"s3a://projet-poc-aida/rp\"\n",
    "parquet_file_name=\"individus_snappy_parquet\"\n",
    "data_path=f\"{work_dir}/{parquet_file_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab0756-bd7a-41f6-bf38-70289420610f",
   "metadata": {},
   "source": [
    "# Step 1: Prepare source dataframe\n",
    "\n",
    "Use spark context to read a parquet file and return a data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1731e55-b6f7-4b93-ae85-0695405013e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04 10:30:29,421 INFO datasources.InMemoryFileIndex: It took 49 ms to list leaf files for 1 paths.\n",
      "2022-09-04 10:30:29,479 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "2022-09-04 10:30:29,480 INFO scheduler.DAGScheduler: Got job 5 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2022-09-04 10:30:29,480 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "2022-09-04 10:30:29,480 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-09-04 10:30:29,481 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-09-04 10:30:29,482 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2022-09-04 10:30:29,503 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 157.1 KiB, free 19.0 GiB)\n",
      "2022-09-04 10:30:29,506 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 58.6 KiB, free 19.0 GiB)\n",
      "2022-09-04 10:30:29,507 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on jupyter-4061-0.jupyter-4061.user-pengfei.svc.cluster.local:35893 (size: 58.6 KiB, free: 19.0 GiB)\n",
      "2022-09-04 10:30:29,507 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\n",
      "2022-09-04 10:30:29,508 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2022-09-04 10:30:29,508 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "2022-09-04 10:30:29,509 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (10.233.127.151, executor 8, partition 0, PROCESS_LOCAL, 4684 bytes) taskResourceAssignments Map()\n",
      "2022-09-04 10:30:29,534 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.233.127.151:41249 (size: 58.6 KiB, free: 9.1 GiB)\n",
      "2022-09-04 10:30:29,696 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 187 ms on 10.233.127.151 (executor 8) (1/1)\n",
      "2022-09-04 10:30:29,696 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "2022-09-04 10:30:29,697 INFO scheduler.DAGScheduler: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.215 s\n",
      "2022-09-04 10:30:29,697 INFO scheduler.DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-09-04 10:30:29,697 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "2022-09-04 10:30:29,698 INFO scheduler.DAGScheduler: Job 5 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.218988 s\n"
     ]
    }
   ],
   "source": [
    "df_parquet=spark.read.parquet(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74a6b079-e3d1-4503-9c30-aa11d888d8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------+-----------------+--------------+-------------------+---------------+------------------+-------------+--------------+------------------+----+----------+-------+----------------+-------+---+---------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n",
      "|region_residence|departement_residence|commune_residence|region_travail|departement_travail|commune_travail|commune_anterieure|commune_etude|pays_naissance|             poids|sexe|statut_pro|densite|recherche_emploi|diplome|age|     variable00|variable01|variable02|variable03|variable04|variable05|variable06|variable07|variable08|variable09|variable10|variable11|variable12|variable13|variable14|variable15|variable16|variable17|variable18|variable19|variable20|variable21|variable22|variable23|variable24|variable25|variable26|variable27|variable28|variable29|variable30|variable31|variable32|variable33|variable34|variable35|variable36|variable37|variable38|variable39|variable40|variable41|variable42|variable43|variable44|variable45|variable46|variable47|variable48|variable49|\n",
      "+----------------+---------------------+-----------------+--------------+-------------------+---------------+------------------+-------------+--------------+------------------+----+----------+-------+----------------+-------+---+---------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n",
      "|              44|                   51|            51265|            44|                 51|          51265|             51265|        51265|            31|1.2622272766699092|   2|         Z|      3|               2|      0|  4|930660000002340|         1|         6|      9316|  652242.5| 6871102.2|         2|      2017|         1|      1989|         5|         9|     28027|        27|        27|         2|     75113|       999|         1|        11|         2|         2|       111|       111|         1|       ZZZ|       997|         0|       997|         1|         2|         2|         Z|     YYYYY|     ZZZZZ|         2|     93055|     99999|        16|        16|        1G|         Z|         1|         0| 930660902|         1|     93066|       999|       999|         6|\n",
      "|              28|                   61|            61493|            28|                 61|          61493|             61493|        61493|            99| 0.829950240429891|   1|         Z|      4|               0|      1|  6|930660000002340|         1|         6|      9316|  652242.5| 6871102.2|         2|      2017|         1|      1989|         5|         9|     28027|        27|        27|         2|     75113|       999|         1|        11|         2|         2|       111|       111|         1|       ZZZ|       997|         0|       997|         1|         2|         2|         Z|     YYYYY|     ZZZZZ|         2|     93055|     99999|        16|        16|        1G|         Z|         1|         0| 930660902|         1|     93066|       999|       999|         6|\n",
      "|              32|                   59|            59019|            32|                 59|          59019|             59019|        59019|            51|1.1022629424168937|   2|         Z|      3|               1|      4|  6|930660000002340|         1|         6|      9316|  652242.5| 6871102.2|         2|      2017|         1|      1989|         5|         9|     28027|        27|        27|         2|     75113|       999|         1|        11|         2|         2|       111|       111|         1|       ZZZ|       997|         0|       997|         1|         2|         2|         Z|     YYYYY|     ZZZZZ|         2|     93055|     99999|        16|        16|        1G|         Z|         1|         0| 930660902|         1|     93066|       999|       999|         6|\n",
      "|              84|                   74|            74191|            44|                 08|          08191|             74191|        74191|            41|0.5779705758148307|   1|         Z|      3|               0|      6|  3|930660000002340|         1|         6|      9316|  652242.5| 6871102.2|         2|      2017|         1|      1989|         5|         9|     28027|        27|        27|         2|     75113|       999|         1|        11|         2|         2|       111|       111|         1|       ZZZ|       997|         0|       997|         1|         2|         2|         Z|     YYYYY|     ZZZZZ|         2|     93055|     99999|        16|        16|        1G|         Z|         1|         0| 930660902|         1|     93066|       999|       999|         6|\n",
      "|              44|                   10|            10027|            44|                 10|          10027|             10027|        10027|            21|0.6162201484479796|   1|         2|      4|               2|      6|  1|930660000002340|         1|         6|      9316|  652242.5| 6871102.2|         2|      2017|         1|      1989|         5|         9|     28027|        27|        27|         2|     75113|       999|         1|        11|         2|         2|       111|       111|         1|       ZZZ|       997|         0|       997|         1|         2|         2|         Z|     YYYYY|     ZZZZZ|         2|     93055|     99999|        16|        16|        1G|         Z|         1|         0| 930660902|         1|     93066|       999|       999|         6|\n",
      "+----------------+---------------------+-----------------+--------------+-------------------+---------------+------------------+-------------+--------------+------------------+----+----------+-------+----------------+-------+---+---------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_parquet.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b3e5b-ecd3-4cd6-b674-c619c84fbc19",
   "metadata": {},
   "source": [
    "# Step2: Create a table in hive metastore\n",
    "\n",
    "Use the spark dataframe to create a hive table in the hive metastore. So we can reuse it for later. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bae3b20b-ffcb-4860-bd98-b1eb0684d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name=\"individus_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5df66c6-e677-4081-84ab-9c302030a656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04 10:12:23,703 INFO conf.HiveConf: Found configuration file file:/opt/hive/conf/hive-site.xml\n",
      "2022-09-04 10:12:23,732 INFO hive.HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.\n",
      "2022-09-04 10:12:23,962 INFO client.HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/home/jovyan/work/Poc_Aida/notebook/spark-warehouse\n",
      "2022-09-04 10:12:24,015 INFO hive.metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083\n",
      "2022-09-04 10:12:24,034 INFO hive.metastore: Opened a connection to metastore, current connections: 1\n",
      "2022-09-04 10:12:24,092 INFO hive.metastore: Connected to metastore.\n",
      "2022-09-04 10:12:24,454 INFO sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=5efb23be-b125-4a4b-806a-c66fd63fb1c6, clientType=HIVECLI]\n",
      "2022-09-04 10:12:24,456 WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "2022-09-04 10:12:24,457 INFO hive.metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook\n",
      "2022-09-04 10:12:24,460 INFO hive.metastore: Closed a connection to metastore, current connections: 0\n",
      "2022-09-04 10:12:24,463 INFO hive.metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083\n",
      "2022-09-04 10:12:24,464 INFO hive.metastore: Opened a connection to metastore, current connections: 1\n",
      "2022-09-04 10:12:24,467 INFO hive.metastore: Connected to metastore.\n",
      "2022-09-04 10:12:24,569 INFO hive.metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083\n",
      "2022-09-04 10:12:24,570 INFO hive.metastore: Opened a connection to metastore, current connections: 2\n",
      "2022-09-04 10:12:24,571 INFO hive.metastore: Connected to metastore.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_str = ', '.join([' '.join(x) for x in df_parquet.dtypes])\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {table_name}\n",
    "({schema_str})\n",
    "STORED as parquet LOCATION '{data_path}'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbe9ee03-c83a-43db-b8a3-1a65f3c2c282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04 10:12:29,515 INFO codegen.CodeGenerator: Code generated in 31.947752 ms\n",
      "2022-09-04 10:12:29,572 INFO codegen.CodeGenerator: Code generated in 8.157305 ms\n",
      "2022-09-04 10:12:29,592 INFO codegen.CodeGenerator: Code generated in 8.772728 ms\n",
      "2022-09-04 10:12:29,604 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2022-09-04 10:12:29,605 INFO scheduler.DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2022-09-04 10:12:29,606 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2022-09-04 10:12:29,606 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-09-04 10:12:29,606 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-09-04 10:12:29,607 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2022-09-04 10:12:29,612 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.5 KiB, free 19.0 GiB)\n",
      "2022-09-04 10:12:29,616 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 19.0 GiB)\n",
      "2022-09-04 10:12:29,616 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on jupyter-4061-0.jupyter-4061.user-pengfei.svc.cluster.local:39841 (size: 3.8 KiB, free: 19.0 GiB)\n",
      "2022-09-04 10:12:29,617 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\n",
      "2022-09-04 10:12:29,618 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2022-09-04 10:12:29,618 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "2022-09-04 10:12:29,624 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.233.112.158, executor 9, partition 0, PROCESS_LOCAL, 4684 bytes) taskResourceAssignments Map()\n",
      "2022-09-04 10:12:29,659 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.233.112.158:38863 (size: 3.8 KiB, free: 9.4 GiB)\n",
      "2022-09-04 10:12:29,688 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 68 ms on 10.233.112.158 (executor 9) (1/1)\n",
      "2022-09-04 10:12:29,688 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "2022-09-04 10:12:29,689 INFO scheduler.DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.080 s\n",
      "2022-09-04 10:12:29,689 INFO scheduler.DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-09-04 10:12:29,689 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "2022-09-04 10:12:29,690 INFO scheduler.DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.085691 s\n",
      "2022-09-04 10:12:29,703 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2022-09-04 10:12:29,704 INFO scheduler.DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2022-09-04 10:12:29,704 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2022-09-04 10:12:29,704 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-09-04 10:12:29,704 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-09-04 10:12:29,705 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2022-09-04 10:12:29,708 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.5 KiB, free 19.0 GiB)\n",
      "2022-09-04 10:12:29,711 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 19.0 GiB)\n",
      "2022-09-04 10:12:29,712 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on jupyter-4061-0.jupyter-4061.user-pengfei.svc.cluster.local:39841 (size: 3.8 KiB, free: 19.0 GiB)\n",
      "2022-09-04 10:12:29,713 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\n",
      "2022-09-04 10:12:29,714 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))\n",
      "2022-09-04 10:12:29,714 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "2022-09-04 10:12:29,715 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (10.233.112.158, executor 9, partition 1, PROCESS_LOCAL, 4684 bytes) taskResourceAssignments Map()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+\n",
      "|namespace|     tableName|isTemporary|\n",
      "+---------+--------------+-----------+\n",
      "|  default|     individus|      false|\n",
      "|  default|individus_test|      false|\n",
      "+---------+--------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04 10:12:29,736 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.233.112.158:38863 (size: 3.8 KiB, free: 9.4 GiB)\n",
      "2022-09-04 10:12:29,750 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 36 ms on 10.233.112.158 (executor 9) (1/1)\n",
      "2022-09-04 10:12:29,750 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "2022-09-04 10:12:29,751 INFO scheduler.DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.044 s\n",
      "2022-09-04 10:12:29,751 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-09-04 10:12:29,751 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "2022-09-04 10:12:29,751 INFO scheduler.DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0.048112 s\n",
      "2022-09-04 10:12:29,765 INFO codegen.CodeGenerator: Code generated in 11.346359 ms\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables;').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155fbab0-b7ab-425b-b187-bada38a8b5fd",
   "metadata": {},
   "source": [
    "Now your hive table has been created. In the backgroud, if you enabled the listener, the metadata of this hive table will be uploaded to our [data catalog](https://atlas.lab.sspcloud.fr/index.html#!/search). So you can find all your hive table easily even you don't have notebook anymore.\n",
    "\n",
    "You can try to use the search engine of our [data catalog](https://atlas.lab.sspcloud.fr/index.html#!/search) to find your table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4480a72-9c1c-4d91-bb30-2b7503d0b66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:====================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------+-----------------+--------------+-------------------+---------------+------------------+-------------+--------------+------------------+----+----------+-------+----------------+-------+---+---------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n",
      "|region_residence|departement_residence|commune_residence|region_travail|departement_travail|commune_travail|commune_anterieure|commune_etude|pays_naissance|             poids|sexe|statut_pro|densite|recherche_emploi|diplome|age|     variable00|variable01|variable02|variable03|variable04|variable05|variable06|variable07|variable08|variable09|variable10|variable11|variable12|variable13|variable14|variable15|variable16|variable17|variable18|variable19|variable20|variable21|variable22|variable23|variable24|variable25|variable26|variable27|variable28|variable29|variable30|variable31|variable32|variable33|variable34|variable35|variable36|variable37|variable38|variable39|variable40|variable41|variable42|variable43|variable44|variable45|variable46|variable47|variable48|variable49|\n",
      "+----------------+---------------------+-----------------+--------------+-------------------+---------------+------------------+-------------+--------------+------------------+----+----------+-------+----------------+-------+---+---------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n",
      "|              44|                   51|            51265|            44|                 51|          51265|             51265|        51265|            31|1.2622272766699092|   2|         Z|      3|               2|      0|  4|930660000002340|         1|         6|      9316|  652242.5| 6871102.2|         2|      2017|         1|      1989|         5|         9|     28027|        27|        27|         2|     75113|       999|         1|        11|         2|         2|       111|       111|         1|       ZZZ|       997|         0|       997|         1|         2|         2|         Z|     YYYYY|     ZZZZZ|         2|     93055|     99999|        16|        16|        1G|         Z|         1|         0| 930660902|         1|     93066|       999|       999|         6|\n",
      "|              28|                   61|            61493|            28|                 61|          61493|             61493|        61493|            99| 0.829950240429891|   1|         Z|      4|               0|      1|  6|930660000002340|         1|         6|      9316|  652242.5| 6871102.2|         2|      2017|         1|      1989|         5|         9|     28027|        27|        27|         2|     75113|       999|         1|        11|         2|         2|       111|       111|         1|       ZZZ|       997|         0|       997|         1|         2|         2|         Z|     YYYYY|     ZZZZZ|         2|     93055|     99999|        16|        16|        1G|         Z|         1|         0| 930660902|         1|     93066|       999|       999|         6|\n",
      "|              32|                   59|            59019|            32|                 59|          59019|             59019|        59019|            51|1.1022629424168937|   2|         Z|      3|               1|      4|  6|930660000002340|         1|         6|      9316|  652242.5| 6871102.2|         2|      2017|         1|      1989|         5|         9|     28027|        27|        27|         2|     75113|       999|         1|        11|         2|         2|       111|       111|         1|       ZZZ|       997|         0|       997|         1|         2|         2|         Z|     YYYYY|     ZZZZZ|         2|     93055|     99999|        16|        16|        1G|         Z|         1|         0| 930660902|         1|     93066|       999|       999|         6|\n",
      "|              84|                   74|            74191|            44|                 08|          08191|             74191|        74191|            41|0.5779705758148307|   1|         Z|      3|               0|      6|  3|930660000002340|         1|         6|      9316|  652242.5| 6871102.2|         2|      2017|         1|      1989|         5|         9|     28027|        27|        27|         2|     75113|       999|         1|        11|         2|         2|       111|       111|         1|       ZZZ|       997|         0|       997|         1|         2|         2|         Z|     YYYYY|     ZZZZZ|         2|     93055|     99999|        16|        16|        1G|         Z|         1|         0| 930660902|         1|     93066|       999|       999|         6|\n",
      "|              44|                   10|            10027|            44|                 10|          10027|             10027|        10027|            21|0.6162201484479796|   1|         2|      4|               2|      6|  1|930660000002340|         1|         6|      9316|  652242.5| 6871102.2|         2|      2017|         1|      1989|         5|         9|     28027|        27|        27|         2|     75113|       999|         1|        11|         2|         2|       111|       111|         1|       ZZZ|       997|         0|       997|         1|         2|         2|         Z|     YYYYY|     ZZZZZ|         2|     93055|     99999|        16|        16|        1G|         Z|         1|         0| 930660902|         1|     93066|       999|       999|         6|\n",
      "+----------------+---------------------+-----------------+--------------+-------------------+---------------+------------------+-------------+--------------+------------------+----+----------+-------+----------------+-------+---+---------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"SELECT * FROM {table_name} limit 5\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef6130d-146e-47fc-b264-a08418a33f9c",
   "metadata": {},
   "source": [
    "# Step 3. Drop table \n",
    "\n",
    "You can delete your table if you don't need it anymore. You will notice the metadata of the deleted table are `removed` from the data catalog too.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f4d2753-af8b-4203-b4c4-bd79b7c39954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  default|individus|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"drop table if exists {table_name}\"\"\").show()\n",
    "spark.sql('show tables;').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e1c0d2-4dbf-447a-b80b-34a52d2d07a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
